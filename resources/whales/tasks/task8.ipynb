{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "Implement a PyTorch dataset for keypoint detection.\n",
    "\n",
    "Read about custom datasets here:\n",
    "* https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/\n",
    "\n",
    "Image augmentation is an important part of deep learning pipelines. It artificially increases your training sample by generating transformed versions of images.\n",
    "\n",
    "<img src=\"static/imgaug.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "You can read about it here:\n",
    "* https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "* https://github.com/aleju/imgaug\n",
    "\n",
    "You should implement the following augmentations:\n",
    "* randomly fliping left and right\n",
    "* randomly fliping up and down\n",
    "* randomly translating by up to 4 pixels\n",
    "* randomly rotating the image by 180 degrees\n",
    "* randomly scaling the image from 1.0 to 1.5\n",
    "\n",
    "Apart from reading images and augmenting, the loader is also cropping the input image by using outputs of the localizer network (bounding box coordinates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Solution\n",
    "Your solution function should be called solution. In this case we leave it for consistency but you don't need to do anything with it. \n",
    "\n",
    "CONFIG is a dictionary with all parameters that you want to pass to your solution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution():\n",
    "    return DatasetAligner\n",
    "\n",
    "class DatasetAligner(Dataset):\n",
    "    def __init__(self, X, y, crop_coordinates, img_dirpath, augmentation, target_size, bins_nr):\n",
    "        super().__init__()\n",
    "        self.X = X.reset_index(drop=True)\n",
    "        self.y = y.reset_index(drop=True)\n",
    "        self.crop_coordinates = crop_coordinates\n",
    "\n",
    "        self.img_dirpath = img_dirpath\n",
    "        self.target_size = target_size\n",
    "        self.bins_nr = bins_nr\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def load_image(self, img_name):\n",
    "        \"\"\"\n",
    "        Read image from disk to numpy array\n",
    "        \"\"\"\n",
    "        return img_array\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Determine the length of the dataset\n",
    "        \"\"\"\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        This method should take the image filepath at X[index] and targets at y[index] and \n",
    "        preprocess them. Use your aligner_preprocessing function.\n",
    "\n",
    "        Xi_tensor: is a torch.FloatTensor for image\n",
    "        yi_tensors: is a torch.LongTensor for targets it's shape should be 1 x k where k is the number of outputs\n",
    "        \"\"\"\n",
    "        return Xi_tensor, yi_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligner_preprocessing(img, target, crop_coordinates, augmentation, *, org_size, target_size, bins_nr):\n",
    "    \"\"\"\n",
    "    Run augmentations and transformations on image and target\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_image, processed_target = crop_image_and_adjust_target(img, target, crop_coordinates)\n",
    "    \n",
    "    if augmentation:\n",
    "        \"\"\"\n",
    "        Run augmentations on Image (and target if needed)       \n",
    "        \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Transform coordinates to bin numbers as explained below and normalize the image\n",
    "    \"\"\"\n",
    "    processed_target = bin_quantizer(processed_target, (height, width), bins_nr)\n",
    "    processed_image = normalize_img(processed_image)\n",
    "    return processed_image, processed_target\n",
    "\n",
    "def crop_image_and_adjust_target(img, target, crop_coordinates):\n",
    "    \"\"\"\n",
    "    crop image by using localization network predictions. \n",
    "    Remember to adjust the keypoint positions to the cropped image\n",
    "    \"\"\"\n",
    "    return cropped_image, adjusted_target\n",
    "\n",
    "def bin_quantizer(coordinates, shape, bins_nr):\n",
    "    \"\"\"\n",
    "    Quantize the height and width and transform coordinates to bin numbers\n",
    "    \"\"\"\n",
    "    return binned_coordinates\n",
    "\n",
    "def normalize_img(img):\n",
    "    mean = [0.28201905, 0.37246801, 0.42341868]\n",
    "    std = [0.13609867, 0.12380088, 0.13325344]\n",
    "    \n",
    "    \"\"\"\n",
    "    Normalize Image\n",
    "    \"\"\"\n",
    "    return normalized_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "64px",
    "width": "255px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
